{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---IMPORTS---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. First look at the data**\n",
    "Here we import the data, look at the column names, the data types. We may deal with quick fixes like null values, empty or useless columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---READING EXPORTING---\n",
    "df=pd.read_csv('data.csv') # to read a csv file using pandas\n",
    "df = pd.read_excel('data.xlsx', sheet_name=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---EXPLORING DATA NATURE---\n",
    "df.columns # To print column names\n",
    "df.dtypes # To print column dtypes\n",
    "\n",
    "df=df.drop(columns=['uuid']) # to remove columns you don't need\n",
    "df['date']=pd.to_datetime(df['date']) # when a date column does not have the correct dtype\n",
    "df['total']=pd.to_numeric(df['total']) # when a numeric column does not have the correct dtype\n",
    "df['Invoice'] = df['Invoice'].astype('str')\n",
    "\n",
    "df.head() # print 1st 5 rows of df\n",
    "df.tail() # print last 5 rows of df\n",
    "df.describe() # print an statistical summary of the numeric columns data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---NULL VALUES---\n",
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=False)\n",
    "df.isnull().sum() #to confirm the heatmap\n",
    "\n",
    "df=df.dropna() #drops all rows that have at least one missing value\n",
    "df=df.dropna(axis=1) #drops all columns that have at least one null value\n",
    "df=df.dropna(how='all') #drops all rows or columns that are 100% null\n",
    "df=df.dropna(thresh=3) #drops all rows with 3 or more null values\n",
    "df=df.dropna(subset=['target_column']) #drops rows that have a null value in this column\n",
    "\n",
    "df['column_with_nulls']=df['column_with_nulls'].fillna(0) #fills null values of that column with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---REMOVE DUPLICATES---\n",
    "df=df.loc[~df.duplicated(subset=['column1', 'column2'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---REPLACE ANYTHING---\n",
    "df['col1'] = df['col1'].replace(0,None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DATA MANIPULATION ----\n",
    "\n",
    "df[['col1', 'col2']] # select columns\n",
    "\n",
    "df[df['col'] > 5] # where condition or filtering\n",
    "df[(df['col1'] > 5) & (df['col2'] < 10)]\n",
    "df[(df['col1'] > 5) | (df['col2'] < 10)]\n",
    "\n",
    "aggregated_df = df.groupby(by = 'Customer ID', as_index = False).agg( # grouping\n",
    "    MonetaryValue = ('SalesLineTotal', \"sum\"),             # Total money customer has spent\n",
    "    Frequency = ('Invoice', \"nunique\"),                    # Total purchases the customer has made\n",
    "    LastInvoiceDate = ('InvoiceDate', \"max\"),              # Date of last purchase of the customer\n",
    "    FirstInvoiceDate = ('InvoiceDate', \"min\"),             # Date of the first purchase the customer did\n",
    ")\n",
    "df.sort_values('col') # sorting data\n",
    "df.sort_values('col', ascending=False)\n",
    "\n",
    "pd.merge(df1, df2, on='key') # how to join tables\n",
    "pd.merge(df1, df2, on='key', how='left')\n",
    "pd.merge(df1, df2, on='key', how='right')\n",
    "pd.merge(df1, df2, on='key', how='outer')\n",
    "\n",
    "df.loc[len(df)] = df.append(...) # add row\n",
    "\n",
    "\n",
    "# Row number per department based on score descending\n",
    "df['row_number'] = df.sort_values(['department', 'score'], ascending=[True, False]) \\\n",
    "                     .groupby('department') \\\n",
    "                     .cumcount() + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Data Exploration**\n",
    "Now we can do our 1st deep analysis of the data, we'll want to look carefully at each of the variables to find patterns that could be useful when preparing the data, removing outliers or for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints a matrix of scatter plots and histograms looking at all the columns at the same time\n",
    "sns.pairplot(data=df, vars=['meter_hours', 'category', 'manufacturer', 'region', 'price_usd', 'year'], hue='is_new')\n",
    "plt.show()\n",
    "\n",
    "# Looking at an individual scatter plot\n",
    "ax=sns.scatterplot(data=df,x='meter_hours', y='price_usd', hue='is_new')\n",
    "ax.set_title('price vs meter_hours')\n",
    "plt.show()\n",
    "\n",
    "# Looking at an individual distribution plot\n",
    "ax=sns.displot(data=df,x='meter_hours', hue='is_new', binwidth=2000)\n",
    "ax.fig.suptitle('meter_hours distribution')\n",
    "plt.show()\n",
    "\n",
    "# Looking at a box plot\n",
    "ax=sns.boxplot(data=df, y='meter_hours', x='is_new')\n",
    "ax.set_title('meter_hours box plot')\n",
    "plt.show()\n",
    "\n",
    "# Looking at a correlation heatmap\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "# Creating pivot plotss\n",
    "pivot=pd.pivot_table(df, values='var_to_aggregate', index='what_you_are_groupingby', columns='what_you_want_to_segment_by', \n",
    "                     aggfunc=np.sum)\n",
    "pivot.plot.line(figsize=(10,7), title='title', ylabel='Name')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Data Preparation and Feature Engineering**\n",
    "After getting to know the data, we have a better idea of which outliers to remove and what new variables que can derive from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---OUTLIERS---\n",
    "df=df.loc[(df['meter_hours'] < df.meter_hours.quantile(0.992))].copy() #very particular way for removing some outliers of a potentially problematic column\n",
    "df=df.loc[(df['price_usd'] < df.price_usd.quantile(0.95))].copy()\n",
    "\n",
    "# or you can also use the Interquantile Range Method}\n",
    "M_Q1 = df[\"MonetaryValue\"].quantile(0.25)\n",
    "M_Q3 = df[\"MonetaryValue\"].quantile(0.75)\n",
    "M_IQR = M_Q3 - M_Q1\n",
    "\n",
    "monetary_outliers_df = df[(df[\"MonetaryValue\"] > (M_Q3 + 1.5 * M_IQR)) | (df[\"MonetaryValue\"] < (M_Q1 - 1.5 * M_IQR))].copy()\n",
    "\n",
    "\n",
    "#--- WORKING WITH DATES ----\n",
    "DF['Recency'] = (DF - aggregated_df['LastInvoiceDate']).dt.days # How recently has the customer made its latest purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---QUERY IN PYTHON---\n",
    "\n",
    "# You can do complex queries this way if its easier for you to do it\n",
    "\n",
    "from pandasql import sqldf\n",
    "\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "\n",
    "query='''\n",
    "select *, \n",
    "(case when meter_hours <= 10000 then meter_hours*meter_hours else meter_hours end) as meter_hours_sq\n",
    "from df\n",
    "'''\n",
    "\n",
    "df=pysqldf(query)\n",
    "df['meter_hours_log'] = np.where(df['meter_hours'] > 1,np.log10(df['meter_hours']),np.nan)\n",
    "df['year_sqrt'] = np.sqrt(df['year']).astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Model Creation**\n",
    "From the nature of the problem you can identify which kind of algorithm you need (clasificator, regressor, clustering, anomalies, etc.) and with the data analysis you can know  which one of the many options of regressors to choose (in case it's a regressor): linear regression, XGBoostRegressor, lightGBMRegressor, DecisionTrees, RandomForest, etc.\n",
    "<br/>\n",
    "<br/>\n",
    "We'll also experiment with different combinations for the hyperparameters of the models which will differ depending on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---PREPARE TEST AND TRAIN DATA---\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x=df.drop(['price_usd'], axis=1)\n",
    "y=df.price_usd\n",
    "\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3,random_state=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---USING XGBRegressor---\n",
    "import xgboost as xgb\n",
    "\n",
    "model=xgb.XGBRegressor(eta=0.2, max_depth=10, n_estimators=500)\n",
    "model.fit(x_train.drop(columns=['meter_hours_sq', 'meter_hours_log']), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(x_test.drop(columns=['meter_hours_sq', 'meter_hours_log']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Model Evaluation**\n",
    "There are different evaluation metrics that we can use depending on the nature of the algorithm the situation needs you to use. \n",
    "<br/>\n",
    "<br/>\n",
    "For regressors: R square, MSE, MAE, LMSQE.\n",
    "<br/>\n",
    "For classificators: Accuracy, precision, recall, ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "print(\"EVALUATION FOR MODEL WITH ORIGINAL VARIABLES\")\n",
    "# Calculate the mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate the mean absolute error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "# Calculate the R-squared value\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now steps 5 and 6 but for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load sample dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))  # e.g., Accuracy: 0.9561\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "xgb.plot_importance(model, max_num_features=10)\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
