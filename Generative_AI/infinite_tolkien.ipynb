{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8413065",
   "metadata": {},
   "source": [
    "# **Infinite Tolkien Generator**\n",
    "\n",
    "In this nb we'll develop a NLP model with transformers architecture to showcase the core functionality of LLMs like ChatGPT. We'll train a neural network with the entire text of the Fellowship Of The Ring book from Tolkien and it will be able to generate Tolkien like text without limit. This model is character level language model, not yet a \"sub-word\" predictor but it is still an extremely educational project to start familiarizing with how LLMs are built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce76798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd92b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lotr_fotr.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76cdd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1021057\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ddf49",
   "metadata": {},
   "source": [
    "Lets look at the first 500 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e1d6e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three Rings for the Elven-kings under the sky,\n",
      "               Seven for the Dwarf-lords in their halls of stone,\n",
      "            Nine for Mortal Men doomed to die,\n",
      "              One for the Dark Lord on his dark throne\n",
      "           In the Land of Mordor where the Shadows lie.\n",
      "               One Ring to rule them all, One Ring to find them,\n",
      "               One Ring to bring them all and in the darkness bind them\n",
      "           In the Land of Mordor where the Shadows lie.\n",
      "           \n",
      "FOREWORD\n",
      "\n",
      "This tale grew\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf3624",
   "metadata": {},
   "source": [
    "Here is a list of all the unique characters in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "711f5d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of characters in text : ['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ó', 'á', 'â', 'ä', 'é', 'ë', 'í', 'ó', 'ú', 'û', '–']\n",
      "A total of 90 different characters\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"List of characters in text :\", chars)\n",
    "print(f'A total of {vocab_size} different characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4273172",
   "metadata": {},
   "source": [
    "Create a mapping from characters to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5227dfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 57, 64, 64, 67, 1, 37, 70, 9, 1, 30, 70, 67, 56, 67]\n",
      "Hello Mr. Frodo\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a dictionary that maps each character to a unique integer index.\n",
    "# This is useful for converting strings into numerical representations.\n",
    "stoi = {}  # 'stoi' stands for \"string to integer\"\n",
    "for i, ch in enumerate(chars):\n",
    "    stoi[ch] = i  # Map character 'ch' to its index 'i'\n",
    "\n",
    "# Step 2: Create a reverse dictionary that maps each index back to its character.\n",
    "# This allows you to convert numerical data back into readable text.\n",
    "itos = {}  # 'itos' stands for \"integer to string\"\n",
    "for i, ch in enumerate(chars):\n",
    "    itos[i] = ch  # Map index 'i' back to character 'ch'\n",
    "\n",
    "# Step 3: Define an encoder function.\n",
    "# This function takes a string and returns a list of integers based on the 'stoi' mapping.\n",
    "def encode(s):\n",
    "    encoded_list = []\n",
    "    for c in s:\n",
    "        encoded_list.append(stoi[c])  # Convert each character to its corresponding integer\n",
    "    return encoded_list\n",
    "\n",
    "# Step 4: Define a decoder function.\n",
    "# This function takes a list of integers and returns the corresponding string using 'itos'.\n",
    "def decode(l):\n",
    "    decoded_string = ''\n",
    "    for i in l:\n",
    "        decoded_string += itos[i]  # Convert each integer back to its corresponding character\n",
    "    return decoded_string\n",
    "\n",
    "\n",
    "print(encode(\"Hello Mr. Frodo\"))\n",
    "print(decode(encode(\"Hello Mr. Frodo\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e1e63c",
   "metadata": {},
   "source": [
    "Let us encode the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0a8c09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([44, 60, 70, 57, 57,  1, 42, 61, 66, 59, 71,  1, 58, 67, 70,  1, 72, 60,\n",
      "        57,  1, 29, 64, 74, 57, 66,  8, 63, 61, 66, 59, 71,  1, 73, 66, 56, 57,\n",
      "        70,  1, 72, 60, 57,  1, 71, 63, 77,  7,  0,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1, 43, 57, 74, 57, 66,  1, 58, 67, 70,  1,\n",
      "        72, 60, 57,  1, 28, 75, 53, 70, 58,  8, 64, 67, 70, 56, 71,  1, 61, 66,\n",
      "         1, 72, 60, 57, 61, 70,  1, 60, 53, 64, 64, 71,  1, 67, 58,  1, 71, 72,\n",
      "        67, 66, 57,  7,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 38,\n",
      "        61, 66, 57,  1, 58, 67, 70,  1, 37, 67, 70, 72, 53, 64,  1, 37, 57, 66,\n",
      "         1, 56, 67, 67, 65, 57, 56,  1, 72, 67,  1, 56, 61, 57,  7,  0,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 39, 66, 57,  1, 58, 67,\n",
      "        70,  1, 72, 60, 57,  1, 28, 53, 70, 63,  1, 36, 67, 70, 56,  1, 67, 66,\n",
      "         1, 60, 61, 71,  1, 56, 53, 70, 63,  1, 72, 60, 70, 67, 66, 57,  0,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 33, 66,  1, 72, 60, 57,  1, 36,\n",
      "        53, 66, 56,  1, 67, 58,  1, 37, 67, 70, 56, 67, 70,  1, 75, 60, 57, 70,\n",
      "        57,  1, 72, 60, 57,  1, 43, 60, 53, 56, 67, 75, 71,  1, 64, 61, 57,  9,\n",
      "         0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 39, 66,\n",
      "        57,  1, 42, 61, 66, 59,  1, 72, 67,  1, 70, 73, 64, 57,  1, 72, 60, 57,\n",
      "        65,  1, 53, 64, 64,  7,  1, 39, 66, 57,  1, 42, 61, 66, 59,  1, 72, 67,\n",
      "         1, 58, 61, 66, 56,  1, 72, 60, 57, 65,  7,  0,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1, 39, 66, 57,  1, 42, 61, 66, 59,  1,\n",
      "        72, 67,  1, 54, 70, 61, 66, 59,  1, 72, 60, 57, 65,  1, 53, 64, 64,  1,\n",
      "        53, 66, 56,  1, 61, 66,  1, 72, 60, 57,  1, 56, 53, 70, 63, 66, 57, 71,\n",
      "        71,  1, 54, 61, 66, 56,  1, 72, 60, 57, 65,  0,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1, 33, 66,  1, 72, 60, 57,  1, 36, 53, 66, 56,  1, 67,\n",
      "        58,  1, 37, 67, 70, 56, 67, 70,  1, 75, 60, 57, 70, 57,  1, 72, 60, 57,\n",
      "         1, 43, 60, 53, 56, 67, 75, 71,  1, 64, 61, 57,  9,  0,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  0, 30, 39, 42, 29, 47, 39, 42, 28,  0,  0,\n",
      "        44, 60, 61, 71,  1, 72, 53, 64, 57,  1, 59, 70, 57, 75])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d9a74f",
   "metadata": {},
   "source": [
    "Now we want to split the data into testing and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9825161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cad91b",
   "metadata": {},
   "source": [
    "Let's ilustrate how we build the chunks of the we'll use to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ff5dc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([44, 60, 70, 57, 57,  1, 42, 61])\n",
      "when input is tensor([44]) the target: 60\n",
      "when input is tensor([44, 60]) the target: 70\n",
      "when input is tensor([44, 60, 70]) the target: 57\n",
      "when input is tensor([44, 60, 70, 57]) the target: 57\n",
      "when input is tensor([44, 60, 70, 57, 57]) the target: 1\n",
      "when input is tensor([44, 60, 70, 57, 57,  1]) the target: 42\n",
      "when input is tensor([44, 60, 70, 57, 57,  1, 42]) the target: 61\n",
      "when input is tensor([44, 60, 70, 57, 57,  1, 42, 61]) the target: 66\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "print(train_data[:block_size])\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05a0d770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs of the transformer:\n",
      "torch.Size([4, 256])\n",
      "tensor([[57, 72, 57,  ..., 66,  1, 72],\n",
      "        [58,  1, 75,  ..., 55, 57,  1],\n",
      "        [57,  1, 54,  ..., 76, 61, 72],\n",
      "        [ 1, 72, 60,  ..., 57,  1, 72]], device='cuda:0')\n",
      "targets of the transformer:\n",
      "torch.Size([4, 256])\n",
      "tensor([[72, 57, 70,  ...,  1, 72, 60],\n",
      "        [ 1, 75, 60,  ..., 57,  1, 67],\n",
      "        [ 1, 54, 57,  ..., 61, 72, 61],\n",
      "        [72, 60, 53,  ...,  1, 72, 60]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print('inputs of the transformer:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('targets of the transformer:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1be835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28008539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58c919c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6438cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f6cb8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c2e6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e0b3d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a8a8875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.808154 M parameters\n",
      "step 0: train loss 4.7143, val loss 4.7131\n",
      "step 500: train loss 1.6001, val loss 1.6409\n",
      "step 1000: train loss 1.2723, val loss 1.3497\n",
      "step 1500: train loss 1.1434, val loss 1.2652\n",
      "step 2000: train loss 1.0568, val loss 1.2308\n",
      "step 2500: train loss 0.9911, val loss 1.2104\n",
      "step 2999: train loss 0.9290, val loss 1.2112\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a980d781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             Old of night hound he sighed him like grain\n",
      "            on a shand\n",
      "          and your resolve is upon dooks\n",
      "            bone afternesses were sholly,\n",
      "              and pull up threes latelling before\n",
      "         the fountain bound, easy shell like meadow: seconsat of folk, if words. So cout seemed to best foreign than enough. There was Strider at a countrive entray way that we have come up through thick wolf; they stood now seemed that they mark. It did not have too straight and stup outhward perhaps.\n",
      "     They like desproúlin people-sprace was sent and Caradhras. Avertheless he tranged or two reterrible; then old window on it would felt rest. It subgers for pace they became mostly great and fresent that beams knownhill, and soon the legends of Old Tunnkeland; and one folk in the seatter was the commoney found the road abovalit upon the files between their neck was steel.\n",
      "     He lightely upon a bright the gate, until it had been wept to buidde slafess. The entrant seemed to be closed to foam orders. They had that his s. Even sitting in the little face of Mina 1. Frodo came the River sight of the Hayvingrime, and I was addeed to still todangerous. Thrórien Tom's birthday-party, burst from the North before anywine that was over the queer made for much for eventforest, or waiting anything hoods or deep more two sort accounts, that brought may see fails-tones, and though me for some mile who had the River consted. In the spedaill of mixturel seems be a nadrespectice of young spebate up about likes of said of it, and that is a very beartiang of to belong this for us: age merry sups in talk, and not entime by course; and the mainly means among the origin, or uneather trees, and too mist; but the river and teast of those small befolk countries.'\n",
      "     At the day's Regrewful by the city the heads. As four going forboders could be looking after changed growing on the attention frostice the Sunknelled fell on the chair and the villages of relunches were held, and hi\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5df548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba1487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c2d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
